{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# 체크포인트를 만드는 라이브러리 부분 임포트\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================\n",
    "import random as rn\n",
    "\n",
    "seed_num = 0\n",
    "np.random.seed(seed_num)\n",
    "rn.seed(seed_num)\n",
    "tf.random.set_seed(seed_num)\n",
    "#============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SBAUser\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Default GPU Device: /device:GPU:0\n",
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"No GPU Found\")\n",
    "\n",
    "# Create a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    # Run a simple computation on the GPU\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "        c = tf.matmul(a, b)\n",
    "\n",
    "        # Initialize the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Print the result of the computation\n",
    "        print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "#   try:\n",
    "#     tf.config.set_logical_device_configuration(\n",
    "#         gpus[0],\n",
    "#         [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "#   except RuntimeError as e:\n",
    "#     # 프로그램 시작시에 가상 장치가 설정되어야만 합니다\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_helmet = os.listdir('helmet_generation/With_Helmet/')\n",
    "without_helmet = os.listdir('helmet_generation/Without_Helmet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_label = [(cv2.imread('helmet_generation/With_Helmet/'+i),1) for i in with_helmet] + [(cv2.imread('helmet_generation/Without_Helmet/'+ i),0) for i in without_helmet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.shuffle(image_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i[0] for i in image_data_label]\n",
    "y = [i[1] for i in image_data_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [cv2.imread('./data/with_helmet_resize/'+i) for i in with_helmet] + [cv2.imread('./data/without_helmet_resize/'+ i) for i in without_helmet]\n",
    "# len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x, dtype=object)\n",
    "y = np.array(y, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[[ 94,  94, 101],\n",
       "               [ 84,  83,  88],\n",
       "               [ 72,  69,  73],\n",
       "               ...,\n",
       "               [ 95, 103, 113],\n",
       "               [ 92, 101, 110],\n",
       "               [ 78,  84,  94]],\n",
       "\n",
       "              [[ 92,  90,  99],\n",
       "               [ 84,  82,  87],\n",
       "               [ 73,  68,  74],\n",
       "               ...,\n",
       "               [ 70,  77,  86],\n",
       "               [ 74,  81,  90],\n",
       "               [ 67,  73,  83]],\n",
       "\n",
       "              [[ 92,  90,  97],\n",
       "               [ 86,  83,  89],\n",
       "               [ 73,  68,  74],\n",
       "               ...,\n",
       "               [ 52,  59,  68],\n",
       "               [ 56,  61,  71],\n",
       "               [ 52,  60,  68]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[140, 149, 178],\n",
       "               [158, 166, 195],\n",
       "               [157, 163, 194],\n",
       "               ...,\n",
       "               [103, 106, 127],\n",
       "               [132, 141, 162],\n",
       "               [115, 120, 140]],\n",
       "\n",
       "              [[135, 144, 170],\n",
       "               [153, 163, 189],\n",
       "               [151, 158, 189],\n",
       "               ...,\n",
       "               [124, 126, 144],\n",
       "               [129, 138, 161],\n",
       "               [108, 116, 136]],\n",
       "\n",
       "              [[131, 139, 167],\n",
       "               [148, 157, 183],\n",
       "               [138, 147, 179],\n",
       "               ...,\n",
       "               [126, 123, 133],\n",
       "               [118, 127, 147],\n",
       "               [110, 120, 141]]], dtype=uint8),\n",
       "       array([[[0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               ...,\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0]],\n",
       "\n",
       "              [[0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               ...,\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0]],\n",
       "\n",
       "              [[0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               ...,\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               ...,\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0]],\n",
       "\n",
       "              [[0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               ...,\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0]],\n",
       "\n",
       "              [[0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               ...,\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0],\n",
       "               [0, 0, 0]]], dtype=uint8),\n",
       "       array([[[164, 188, 208],\n",
       "               [165, 188, 209],\n",
       "               [149, 172, 192],\n",
       "               ...,\n",
       "               [149, 168, 177],\n",
       "               [156, 183, 190],\n",
       "               [154, 190, 196]],\n",
       "\n",
       "              [[155, 175, 197],\n",
       "               [157, 180, 199],\n",
       "               [142, 165, 184],\n",
       "               ...,\n",
       "               [158, 181, 187],\n",
       "               [159, 187, 192],\n",
       "               [162, 197, 201]],\n",
       "\n",
       "              [[157, 177, 197],\n",
       "               [159, 179, 199],\n",
       "               [156, 179, 197],\n",
       "               ...,\n",
       "               [168, 193, 199],\n",
       "               [162, 192, 197],\n",
       "               [164, 196, 201]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[127, 170, 181],\n",
       "               [119, 161, 171],\n",
       "               [120, 159, 165],\n",
       "               ...,\n",
       "               [ 94,  70,  72],\n",
       "               [ 96,  72,  74],\n",
       "               [ 96,  74,  73]],\n",
       "\n",
       "              [[120, 162, 172],\n",
       "               [113, 160, 165],\n",
       "               [114, 156, 161],\n",
       "               ...,\n",
       "               [ 94,  70,  72],\n",
       "               [ 95,  71,  72],\n",
       "               [ 93,  71,  73]],\n",
       "\n",
       "              [[114, 156, 166],\n",
       "               [115, 161, 167],\n",
       "               [124, 166, 171],\n",
       "               ...,\n",
       "               [ 94,  70,  72],\n",
       "               [ 92,  70,  72],\n",
       "               [ 93,  71,  73]]], dtype=uint8), ...,\n",
       "       array([[[172, 231, 249],\n",
       "               [164, 221, 239],\n",
       "               [165, 216, 232],\n",
       "               ...,\n",
       "               [132, 190, 246],\n",
       "               [133, 192, 246],\n",
       "               [132, 191, 239]],\n",
       "\n",
       "              [[174, 231, 248],\n",
       "               [164, 221, 235],\n",
       "               [164, 215, 227],\n",
       "               ...,\n",
       "               [133, 190, 246],\n",
       "               [134, 192, 245],\n",
       "               [134, 191, 239]],\n",
       "\n",
       "              [[175, 231, 246],\n",
       "               [165, 220, 230],\n",
       "               [163, 215, 222],\n",
       "               ...,\n",
       "               [134, 189, 245],\n",
       "               [136, 191, 245],\n",
       "               [136, 191, 239]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[ 85, 111, 187],\n",
       "               [ 94, 124, 146],\n",
       "               [ 97, 128, 132],\n",
       "               ...,\n",
       "               [244, 254, 252],\n",
       "               [155, 212, 204],\n",
       "               [108, 140, 147]],\n",
       "\n",
       "              [[ 85, 111, 190],\n",
       "               [ 95, 124, 150],\n",
       "               [ 98, 129, 134],\n",
       "               ...,\n",
       "               [245, 254, 252],\n",
       "               [158, 215, 207],\n",
       "               [111, 143, 149]],\n",
       "\n",
       "              [[ 85, 110, 192],\n",
       "               [ 96, 125, 154],\n",
       "               [ 99, 129, 137],\n",
       "               ...,\n",
       "               [246, 255, 253],\n",
       "               [161, 219, 211],\n",
       "               [114, 146, 153]]], dtype=uint8),\n",
       "       array([[[223, 224, 224],\n",
       "               [234, 233, 233],\n",
       "               [236, 237, 236],\n",
       "               ...,\n",
       "               [208, 209, 209],\n",
       "               [215, 214, 214],\n",
       "               [227, 227, 227]],\n",
       "\n",
       "              [[225, 225, 225],\n",
       "               [235, 235, 234],\n",
       "               [238, 238, 238],\n",
       "               ...,\n",
       "               [226, 226, 226],\n",
       "               [227, 228, 228],\n",
       "               [238, 239, 239]],\n",
       "\n",
       "              [[225, 225, 224],\n",
       "               [236, 236, 236],\n",
       "               [238, 238, 239],\n",
       "               ...,\n",
       "               [238, 237, 238],\n",
       "               [240, 240, 240],\n",
       "               [247, 247, 247]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[ 10,   9,  10],\n",
       "               [ 73,  74,  73],\n",
       "               [165, 165, 164],\n",
       "               ...,\n",
       "               [ 75,  75,  75],\n",
       "               [ 97,  96,  97],\n",
       "               [140, 140, 140]],\n",
       "\n",
       "              [[ 11,  10,  10],\n",
       "               [ 71,  72,  71],\n",
       "               [160, 160, 161],\n",
       "               ...,\n",
       "               [ 82,  81,  81],\n",
       "               [108, 107, 108],\n",
       "               [148, 148, 148]],\n",
       "\n",
       "              [[ 18,  17,  18],\n",
       "               [ 71,  71,  70],\n",
       "               [151, 151, 152],\n",
       "               ...,\n",
       "               [ 94,  94,  94],\n",
       "               [125, 126, 125],\n",
       "               [167, 167, 166]]], dtype=uint8),\n",
       "       array([[[180, 184, 172],\n",
       "               [177, 181, 169],\n",
       "               [166, 171, 160],\n",
       "               ...,\n",
       "               [ 68,  74,  76],\n",
       "               [ 44,  47,  48],\n",
       "               [ 76,  86,  88]],\n",
       "\n",
       "              [[180, 183, 169],\n",
       "               [177, 179, 164],\n",
       "               [164, 168, 154],\n",
       "               ...,\n",
       "               [ 67,  73,  75],\n",
       "               [ 44,  47,  49],\n",
       "               [ 77,  87,  89]],\n",
       "\n",
       "              [[179, 181, 166],\n",
       "               [175, 178, 160],\n",
       "               [163, 165, 149],\n",
       "               ...,\n",
       "               [ 67,  72,  74],\n",
       "               [ 46,  48,  49],\n",
       "               [ 79,  88,  91]],\n",
       "\n",
       "              ...,\n",
       "\n",
       "              [[ 42,  42,  44],\n",
       "               [ 42,  42,  43],\n",
       "               [ 42,  42,  43],\n",
       "               ...,\n",
       "               [ 44,  47,  49],\n",
       "               [ 84,  96, 100],\n",
       "               [137, 150, 155]],\n",
       "\n",
       "              [[ 43,  43,  46],\n",
       "               [ 42,  42,  44],\n",
       "               [ 41,  42,  43],\n",
       "               ...,\n",
       "               [ 44,  47,  49],\n",
       "               [ 84,  95, 100],\n",
       "               [138, 151, 156]],\n",
       "\n",
       "              [[ 49,  52,  58],\n",
       "               [ 47,  49,  54],\n",
       "               [ 45,  47,  51],\n",
       "               ...,\n",
       "               [ 44,  48,  49],\n",
       "               [ 85,  96, 101],\n",
       "               [139, 152, 157]]], dtype=uint8)], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14516\\1327190357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "x = x.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=4096,activation=\"relu\"))\n",
    "# model.add(Dense(units=4096,activation=\"relu\"))\n",
    "# model.add(Dense(units=2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG = VGG16(input_shape=(128, 128, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "VGG.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(VGG)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256,activation=\"relu\"))\n",
    "model.add(Dense(units=128,activation=\"relu\"))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               2097408   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,845,121\n",
      "Trainable params: 16,845,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.6434 - acc: 0.6963"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.27299, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 45s 3ms/sample - loss: 0.6434 - acc: 0.6963 - val_loss: 0.2730 - val_acc: 0.8940\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.3106 - acc: 0.8681\n",
      "Epoch 2: val_loss improved from 0.27299 to 0.17495, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.3106 - acc: 0.8681 - val_loss: 0.1750 - val_acc: 0.9333\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1773 - acc: 0.9311\n",
      "Epoch 3: val_loss improved from 0.17495 to 0.15578, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1773 - acc: 0.9311 - val_loss: 0.1558 - val_acc: 0.9438\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1538 - acc: 0.9417\n",
      "Epoch 4: val_loss improved from 0.15578 to 0.14375, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1538 - acc: 0.9417 - val_loss: 0.1437 - val_acc: 0.9467\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1342 - acc: 0.9504\n",
      "Epoch 5: val_loss improved from 0.14375 to 0.11032, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1342 - acc: 0.9504 - val_loss: 0.1103 - val_acc: 0.9615\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1171 - acc: 0.9573\n",
      "Epoch 6: val_loss did not improve from 0.11032\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1171 - acc: 0.9573 - val_loss: 0.1544 - val_acc: 0.9452\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1047 - acc: 0.9617\n",
      "Epoch 7: val_loss did not improve from 0.11032\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1047 - acc: 0.9617 - val_loss: 0.1173 - val_acc: 0.9578\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1059 - acc: 0.9616\n",
      "Epoch 8: val_loss did not improve from 0.11032\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1059 - acc: 0.9616 - val_loss: 0.1310 - val_acc: 0.9495\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1055 - acc: 0.9606\n",
      "Epoch 9: val_loss improved from 0.11032 to 0.09370, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1055 - acc: 0.9606 - val_loss: 0.0937 - val_acc: 0.9653\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0754 - acc: 0.9724\n",
      "Epoch 10: val_loss improved from 0.09370 to 0.07401, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0754 - acc: 0.9724 - val_loss: 0.0740 - val_acc: 0.9715\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0691 - acc: 0.9743\n",
      "Epoch 11: val_loss improved from 0.07401 to 0.07345, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0691 - acc: 0.9743 - val_loss: 0.0734 - val_acc: 0.9758\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0577 - acc: 0.9787\n",
      "Epoch 12: val_loss did not improve from 0.07345\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0577 - acc: 0.9787 - val_loss: 0.0886 - val_acc: 0.9722\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0666 - acc: 0.9761\n",
      "Epoch 13: val_loss improved from 0.07345 to 0.06908, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0666 - acc: 0.9761 - val_loss: 0.0691 - val_acc: 0.9785\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0499 - acc: 0.9839\n",
      "Epoch 14: val_loss did not improve from 0.06908\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0499 - acc: 0.9839 - val_loss: 0.0786 - val_acc: 0.9718\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0761 - acc: 0.9729\n",
      "Epoch 15: val_loss did not improve from 0.06908\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0761 - acc: 0.9729 - val_loss: 0.1166 - val_acc: 0.9572\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0611 - acc: 0.9792\n",
      "Epoch 16: val_loss improved from 0.06908 to 0.06904, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0611 - acc: 0.9792 - val_loss: 0.0690 - val_acc: 0.9728\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0425 - acc: 0.9847\n",
      "Epoch 17: val_loss did not improve from 0.06904\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0425 - acc: 0.9847 - val_loss: 0.0929 - val_acc: 0.9718\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0508 - acc: 0.9832\n",
      "Epoch 18: val_loss did not improve from 0.06904\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0508 - acc: 0.9832 - val_loss: 0.1173 - val_acc: 0.9600\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0368 - acc: 0.9867\n",
      "Epoch 19: val_loss improved from 0.06904 to 0.06280, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0368 - acc: 0.9867 - val_loss: 0.0628 - val_acc: 0.9793\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1029 - acc: 0.9664\n",
      "Epoch 20: val_loss did not improve from 0.06280\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1029 - acc: 0.9664 - val_loss: 0.1446 - val_acc: 0.9460\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0734 - acc: 0.9731\n",
      "Epoch 21: val_loss improved from 0.06280 to 0.05756, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0734 - acc: 0.9731 - val_loss: 0.0576 - val_acc: 0.9805\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1364 - acc: 0.9511\n",
      "Epoch 22: val_loss did not improve from 0.05756\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1364 - acc: 0.9511 - val_loss: 0.1159 - val_acc: 0.9580\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0628 - acc: 0.9780\n",
      "Epoch 23: val_loss improved from 0.05756 to 0.04771, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0628 - acc: 0.9780 - val_loss: 0.0477 - val_acc: 0.9850\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0404 - acc: 0.9865\n",
      "Epoch 24: val_loss did not improve from 0.04771\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0404 - acc: 0.9865 - val_loss: 0.1154 - val_acc: 0.9653\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0755 - acc: 0.9755\n",
      "Epoch 25: val_loss did not improve from 0.04771\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0755 - acc: 0.9755 - val_loss: 0.0509 - val_acc: 0.9818\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.9912\n",
      "Epoch 26: val_loss did not improve from 0.04771\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0275 - acc: 0.9912 - val_loss: 0.0552 - val_acc: 0.9808\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0251 - acc: 0.9918\n",
      "Epoch 27: val_loss improved from 0.04771 to 0.04430, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0251 - acc: 0.9918 - val_loss: 0.0443 - val_acc: 0.9858\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0247 - acc: 0.9920\n",
      "Epoch 28: val_loss improved from 0.04430 to 0.03851, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0247 - acc: 0.9920 - val_loss: 0.0385 - val_acc: 0.9883\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0148 - acc: 0.9944\n",
      "Epoch 29: val_loss did not improve from 0.03851\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0148 - acc: 0.9944 - val_loss: 0.0536 - val_acc: 0.9815\n",
      "Epoch 30/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9907\n",
      "Epoch 30: val_loss did not improve from 0.03851\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0260 - acc: 0.9907 - val_loss: 0.0506 - val_acc: 0.9825\n",
      "Epoch 31/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0259 - acc: 0.9911\n",
      "Epoch 31: val_loss did not improve from 0.03851\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0259 - acc: 0.9911 - val_loss: 0.0491 - val_acc: 0.9833\n",
      "Epoch 32/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0302 - acc: 0.9894\n",
      "Epoch 32: val_loss did not improve from 0.03851\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0302 - acc: 0.9894 - val_loss: 0.0530 - val_acc: 0.9872\n",
      "Epoch 33/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0263 - acc: 0.9909\n",
      "Epoch 33: val_loss did not improve from 0.03851\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0263 - acc: 0.9909 - val_loss: 0.0532 - val_acc: 0.9847\n",
      "Epoch 34/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0228 - acc: 0.9924\n",
      "Epoch 34: val_loss improved from 0.03851 to 0.02856, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0228 - acc: 0.9924 - val_loss: 0.0286 - val_acc: 0.9925\n",
      "Epoch 35/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0255 - acc: 0.9924\n",
      "Epoch 35: val_loss did not improve from 0.02856\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0255 - acc: 0.9924 - val_loss: 0.0431 - val_acc: 0.9835\n",
      "Epoch 36/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0207 - acc: 0.9932\n",
      "Epoch 36: val_loss improved from 0.02856 to 0.02413, saving model to ./model\\VGG_16_early.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0241 - val_acc: 0.9927\n",
      "Epoch 37/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 37: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0308 - acc: 0.9904 - val_loss: 0.0412 - val_acc: 0.9885\n",
      "Epoch 38/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0232 - acc: 0.9922\n",
      "Epoch 38: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0232 - acc: 0.9922 - val_loss: 0.0833 - val_acc: 0.9803\n",
      "Epoch 39/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0187 - acc: 0.9940\n",
      "Epoch 39: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0187 - acc: 0.9940 - val_loss: 0.0522 - val_acc: 0.9925\n",
      "Epoch 40/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0319 - acc: 0.9904\n",
      "Epoch 40: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0319 - acc: 0.9904 - val_loss: 0.0242 - val_acc: 0.9927\n",
      "Epoch 41/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0130 - acc: 0.9953\n",
      "Epoch 41: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0130 - acc: 0.9953 - val_loss: 0.0277 - val_acc: 0.9935\n",
      "Epoch 42/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9912\n",
      "Epoch 42: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0279 - acc: 0.9912 - val_loss: 0.0491 - val_acc: 0.9827\n",
      "Epoch 43/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0210 - acc: 0.9931\n",
      "Epoch 43: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0210 - acc: 0.9931 - val_loss: 0.0414 - val_acc: 0.9880\n",
      "Epoch 44/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0264 - acc: 0.9911\n",
      "Epoch 44: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0264 - acc: 0.9911 - val_loss: 0.0320 - val_acc: 0.9902\n",
      "Epoch 45/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0184 - acc: 0.9936\n",
      "Epoch 45: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0184 - acc: 0.9936 - val_loss: 0.0403 - val_acc: 0.9883\n",
      "Epoch 46/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9943\n",
      "Epoch 46: val_loss did not improve from 0.02413\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0920 - val_acc: 0.9755\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "modelpath = './model/'\n",
    "if not os.path.exists(modelpath) :\n",
    "    os.mkdir(modelpath)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath + 'VGG_16_early.hdf5', monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience=10)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=[x_test, y_test], epochs = 100,  batch_size = 64, callbacks = [checkpointer, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model = load_model('./model/VGG_16_early.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.024130888986139326, 0.99275]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증셋과 학습셋의 오차 저장\n",
    "\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker = '.', c='red', label = 'Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker = '.', c='blue', label = 'trainset_loss')\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold # k-fold 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : # 1\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.6353 - acc: 0.7703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.22335, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.6353 - acc: 0.7703 - val_loss: 0.2234 - val_acc: 0.9143\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.2035 - acc: 0.9218\n",
      "Epoch 2: val_loss improved from 0.22335 to 0.16169, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.2035 - acc: 0.9218 - val_loss: 0.1617 - val_acc: 0.9390\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1502 - acc: 0.9434\n",
      "Epoch 3: val_loss did not improve from 0.16169\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1502 - acc: 0.9434 - val_loss: 0.3697 - val_acc: 0.8752\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9517\n",
      "Epoch 4: val_loss improved from 0.16169 to 0.14039, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1357 - acc: 0.9517 - val_loss: 0.1404 - val_acc: 0.9488\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1156 - acc: 0.9585\n",
      "Epoch 5: val_loss improved from 0.14039 to 0.12865, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.1156 - acc: 0.9585 - val_loss: 0.1286 - val_acc: 0.9515\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0954 - acc: 0.9659\n",
      "Epoch 6: val_loss improved from 0.12865 to 0.10319, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0954 - acc: 0.9659 - val_loss: 0.1032 - val_acc: 0.9610\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0751 - acc: 0.9734\n",
      "Epoch 7: val_loss improved from 0.10319 to 0.08999, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0751 - acc: 0.9734 - val_loss: 0.0900 - val_acc: 0.9682\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0779 - acc: 0.9721\n",
      "Epoch 8: val_loss did not improve from 0.08999\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0779 - acc: 0.9721 - val_loss: 0.1463 - val_acc: 0.9485\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0670 - acc: 0.9751\n",
      "Epoch 9: val_loss improved from 0.08999 to 0.08725, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0670 - acc: 0.9751 - val_loss: 0.0873 - val_acc: 0.9660\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0630 - acc: 0.9772\n",
      "Epoch 10: val_loss improved from 0.08725 to 0.05950, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0630 - acc: 0.9772 - val_loss: 0.0595 - val_acc: 0.9778\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0486 - acc: 0.9823\n",
      "Epoch 11: val_loss did not improve from 0.05950\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0486 - acc: 0.9823 - val_loss: 0.0792 - val_acc: 0.9725\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0567 - acc: 0.9799\n",
      "Epoch 12: val_loss improved from 0.05950 to 0.05176, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0567 - acc: 0.9799 - val_loss: 0.0518 - val_acc: 0.9818\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0536 - acc: 0.9787\n",
      "Epoch 13: val_loss did not improve from 0.05176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0536 - acc: 0.9787 - val_loss: 0.0722 - val_acc: 0.9730\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0431 - acc: 0.9841\n",
      "Epoch 14: val_loss improved from 0.05176 to 0.05036, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0431 - acc: 0.9841 - val_loss: 0.0504 - val_acc: 0.9830\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0495 - acc: 0.9833\n",
      "Epoch 15: val_loss did not improve from 0.05036\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0495 - acc: 0.9833 - val_loss: 0.0712 - val_acc: 0.9753\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0382 - acc: 0.9862\n",
      "Epoch 16: val_loss did not improve from 0.05036\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0382 - acc: 0.9862 - val_loss: 0.0627 - val_acc: 0.9785\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0511 - acc: 0.9844\n",
      "Epoch 17: val_loss improved from 0.05036 to 0.04917, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0511 - acc: 0.9844 - val_loss: 0.0492 - val_acc: 0.9833\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0452 - acc: 0.9848\n",
      "Epoch 18: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0452 - acc: 0.9848 - val_loss: 0.0545 - val_acc: 0.9825\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0254 - acc: 0.9918\n",
      "Epoch 19: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0254 - acc: 0.9918 - val_loss: 0.0601 - val_acc: 0.9790\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0309 - acc: 0.9893\n",
      "Epoch 20: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0309 - acc: 0.9893 - val_loss: 0.0597 - val_acc: 0.9803\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0369 - acc: 0.9860\n",
      "Epoch 21: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0369 - acc: 0.9860 - val_loss: 0.0770 - val_acc: 0.9750\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0343 - acc: 0.9883\n",
      "Epoch 22: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0343 - acc: 0.9883 - val_loss: 0.0829 - val_acc: 0.9678\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0459 - acc: 0.9843\n",
      "Epoch 23: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0459 - acc: 0.9843 - val_loss: 0.0648 - val_acc: 0.9775\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0491 - acc: 0.9850\n",
      "Epoch 24: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0491 - acc: 0.9850 - val_loss: 0.0587 - val_acc: 0.9800\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0236 - acc: 0.9914\n",
      "Epoch 25: val_loss did not improve from 0.04917\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0236 - acc: 0.9914 - val_loss: 0.0590 - val_acc: 0.9843\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0230 - acc: 0.9927\n",
      "Epoch 26: val_loss improved from 0.04917 to 0.04144, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0230 - acc: 0.9927 - val_loss: 0.0414 - val_acc: 0.9870\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0216 - acc: 0.9926\n",
      "Epoch 27: val_loss improved from 0.04144 to 0.04012, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.0216 - acc: 0.9926 - val_loss: 0.0401 - val_acc: 0.9845\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9909\n",
      "Epoch 28: val_loss did not improve from 0.04012\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0491 - val_acc: 0.9852\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0334 - acc: 0.9878\n",
      "Epoch 29: val_loss did not improve from 0.04012\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0334 - acc: 0.9878 - val_loss: 0.0473 - val_acc: 0.9865\n",
      "Epoch 30/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0222 - acc: 0.9917\n",
      "Epoch 30: val_loss did not improve from 0.04012\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0222 - acc: 0.9917 - val_loss: 0.0486 - val_acc: 0.9860\n",
      "Epoch 31/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0273 - acc: 0.9911\n",
      "Epoch 31: val_loss did not improve from 0.04012\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0448 - val_acc: 0.9872\n",
      "Epoch 32/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0202 - acc: 0.9936\n",
      "Epoch 32: val_loss improved from 0.04012 to 0.03608, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0361 - val_acc: 0.9875\n",
      "Epoch 33/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0349 - acc: 0.9878\n",
      "Epoch 33: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 42s 3ms/sample - loss: 0.0349 - acc: 0.9878 - val_loss: 0.0474 - val_acc: 0.9835\n",
      "Epoch 34/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9960\n",
      "Epoch 34: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 42s 3ms/sample - loss: 0.0136 - acc: 0.9960 - val_loss: 0.1080 - val_acc: 0.9887\n",
      "Epoch 35/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0388 - acc: 0.9878\n",
      "Epoch 35: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0388 - acc: 0.9878 - val_loss: 0.0591 - val_acc: 0.9797\n",
      "Epoch 36/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0480 - acc: 0.9837\n",
      "Epoch 36: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0480 - acc: 0.9837 - val_loss: 0.1082 - val_acc: 0.9650\n",
      "Epoch 37/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0371 - acc: 0.9884\n",
      "Epoch 37: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 42s 3ms/sample - loss: 0.0371 - acc: 0.9884 - val_loss: 0.1477 - val_acc: 0.9503\n",
      "Epoch 38/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0267 - acc: 0.9916\n",
      "Epoch 38: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0267 - acc: 0.9916 - val_loss: 0.0719 - val_acc: 0.9890\n",
      "Epoch 39/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0122 - acc: 0.9962\n",
      "Epoch 39: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0122 - acc: 0.9962 - val_loss: 0.1423 - val_acc: 0.9818\n",
      "Epoch 40/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0270 - acc: 0.9918\n",
      "Epoch 40: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0270 - acc: 0.9918 - val_loss: 0.0735 - val_acc: 0.9843\n",
      "Epoch 41/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0197 - acc: 0.9938\n",
      "Epoch 41: val_loss did not improve from 0.03608\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0197 - acc: 0.9938 - val_loss: 0.7252 - val_acc: 0.8790\n",
      "Epoch 42/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0347 - acc: 0.9893\n",
      "Epoch 42: val_loss improved from 0.03608 to 0.02305, saving model to ./model\\VGG_16_kfold_1.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0347 - acc: 0.9893 - val_loss: 0.0230 - val_acc: 0.9910\n",
      "Epoch 43/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0244 - acc: 0.9933\n",
      "Epoch 43: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0244 - acc: 0.9933 - val_loss: 0.0336 - val_acc: 0.9885\n",
      "Epoch 44/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0162 - acc: 0.9948\n",
      "Epoch 44: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0366 - val_acc: 0.9915\n",
      "Epoch 45/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0145 - acc: 0.9962\n",
      "Epoch 45: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0145 - acc: 0.9962 - val_loss: 0.0499 - val_acc: 0.9920\n",
      "Epoch 46/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0170 - acc: 0.9960\n",
      "Epoch 46: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0170 - acc: 0.9960 - val_loss: 0.0397 - val_acc: 0.9920\n",
      "Epoch 47/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0244 - acc: 0.9917\n",
      "Epoch 47: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0244 - acc: 0.9917 - val_loss: 0.0471 - val_acc: 0.9865\n",
      "Epoch 48/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0387 - acc: 0.9871\n",
      "Epoch 48: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0387 - acc: 0.9871 - val_loss: 0.1484 - val_acc: 0.9655\n",
      "Epoch 49/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.2166 - acc: 0.9277\n",
      "Epoch 49: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.2166 - acc: 0.9277 - val_loss: 0.2206 - val_acc: 0.9455\n",
      "Epoch 50/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.3324 - acc: 0.8840\n",
      "Epoch 50: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.3324 - acc: 0.8840 - val_loss: 0.1906 - val_acc: 0.9247\n",
      "Epoch 51/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.2276 - acc: 0.9161\n",
      "Epoch 51: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.2276 - acc: 0.9161 - val_loss: 0.1347 - val_acc: 0.9532\n",
      "Epoch 52/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1020 - acc: 0.9636\n",
      "Epoch 52: val_loss did not improve from 0.02305\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1020 - acc: 0.9636 - val_loss: 0.0951 - val_acc: 0.9688\n",
      "정확도(accuracy) :  0.991\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fold : # 2\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.6223 - acc: 0.6802\n",
      "Epoch 1: val_loss improved from inf to 0.54439, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.6223 - acc: 0.6802 - val_loss: 0.5444 - val_acc: 0.7448\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.4871 - acc: 0.7584\n",
      "Epoch 2: val_loss improved from 0.54439 to 0.21604, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.4871 - acc: 0.7584 - val_loss: 0.2160 - val_acc: 0.9180\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.2029 - acc: 0.9242\n",
      "Epoch 3: val_loss did not improve from 0.21604\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.2029 - acc: 0.9242 - val_loss: 0.2318 - val_acc: 0.9200\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1652 - acc: 0.9384\n",
      "Epoch 4: val_loss improved from 0.21604 to 0.16484, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1652 - acc: 0.9384 - val_loss: 0.1648 - val_acc: 0.9360\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1461 - acc: 0.9463\n",
      "Epoch 5: val_loss improved from 0.16484 to 0.11917, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1461 - acc: 0.9463 - val_loss: 0.1192 - val_acc: 0.9570\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1045 - acc: 0.9647\n",
      "Epoch 6: val_loss improved from 0.11917 to 0.10969, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1045 - acc: 0.9647 - val_loss: 0.1097 - val_acc: 0.9620\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1053 - acc: 0.9634\n",
      "Epoch 7: val_loss improved from 0.10969 to 0.10912, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1053 - acc: 0.9634 - val_loss: 0.1091 - val_acc: 0.9578\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0944 - acc: 0.9671\n",
      "Epoch 8: val_loss improved from 0.10912 to 0.10585, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0944 - acc: 0.9671 - val_loss: 0.1058 - val_acc: 0.9600\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0747 - acc: 0.9745\n",
      "Epoch 9: val_loss improved from 0.10585 to 0.10272, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0747 - acc: 0.9745 - val_loss: 0.1027 - val_acc: 0.9622\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1947 - acc: 0.9239\n",
      "Epoch 10: val_loss did not improve from 0.10272\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1947 - acc: 0.9239 - val_loss: 0.3208 - val_acc: 0.8675\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1867 - acc: 0.9292\n",
      "Epoch 11: val_loss did not improve from 0.10272\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1867 - acc: 0.9292 - val_loss: 0.1235 - val_acc: 0.9517\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1007 - acc: 0.9630\n",
      "Epoch 12: val_loss did not improve from 0.10272\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1007 - acc: 0.9630 - val_loss: 0.1217 - val_acc: 0.9505\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0691 - acc: 0.9744\n",
      "Epoch 13: val_loss improved from 0.10272 to 0.05511, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0691 - acc: 0.9744 - val_loss: 0.0551 - val_acc: 0.9793\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0543 - acc: 0.9803\n",
      "Epoch 14: val_loss did not improve from 0.05511\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0543 - acc: 0.9803 - val_loss: 0.0714 - val_acc: 0.9743\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0411 - acc: 0.9857\n",
      "Epoch 15: val_loss did not improve from 0.05511\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0411 - acc: 0.9857 - val_loss: 0.0633 - val_acc: 0.9775\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0455 - acc: 0.9843\n",
      "Epoch 16: val_loss improved from 0.05511 to 0.03607, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0455 - acc: 0.9843 - val_loss: 0.0361 - val_acc: 0.9868\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0432 - acc: 0.9855\n",
      "Epoch 17: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0432 - acc: 0.9855 - val_loss: 0.0425 - val_acc: 0.9850\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0396 - acc: 0.9858\n",
      "Epoch 18: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0396 - acc: 0.9858 - val_loss: 0.0465 - val_acc: 0.9822\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0236 - acc: 0.9914\n",
      "Epoch 19: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0236 - acc: 0.9914 - val_loss: 0.0481 - val_acc: 0.9833\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.9899\n",
      "Epoch 20: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0277 - acc: 0.9899 - val_loss: 0.1162 - val_acc: 0.9653\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0259 - acc: 0.9918\n",
      "Epoch 21: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0510 - val_acc: 0.9845\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0220 - acc: 0.9928\n",
      "Epoch 22: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0384 - val_acc: 0.9870\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0243 - acc: 0.9916\n",
      "Epoch 23: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0243 - acc: 0.9916 - val_loss: 0.0552 - val_acc: 0.9833\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0682 - acc: 0.9772\n",
      "Epoch 24: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0682 - acc: 0.9772 - val_loss: 0.0927 - val_acc: 0.9653\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0378 - acc: 0.9866\n",
      "Epoch 25: val_loss did not improve from 0.03607\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0378 - acc: 0.9866 - val_loss: 0.1002 - val_acc: 0.9668\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0298 - acc: 0.9891\n",
      "Epoch 26: val_loss improved from 0.03607 to 0.03491, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0298 - acc: 0.9891 - val_loss: 0.0349 - val_acc: 0.9898\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0381 - acc: 0.9876\n",
      "Epoch 27: val_loss improved from 0.03491 to 0.03454, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0381 - acc: 0.9876 - val_loss: 0.0345 - val_acc: 0.9885\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0113 - acc: 0.9958\n",
      "Epoch 28: val_loss did not improve from 0.03454\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0113 - acc: 0.9958 - val_loss: 0.0353 - val_acc: 0.9895\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0185 - acc: 0.9928\n",
      "Epoch 29: val_loss did not improve from 0.03454\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0185 - acc: 0.9928 - val_loss: 0.0371 - val_acc: 0.9905\n",
      "Epoch 30/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9949\n",
      "Epoch 30: val_loss improved from 0.03454 to 0.02431, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0159 - acc: 0.9949 - val_loss: 0.0243 - val_acc: 0.9925\n",
      "Epoch 31/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0586 - acc: 0.9827\n",
      "Epoch 31: val_loss did not improve from 0.02431\n",
      "16000/16000 [==============================] - 42s 3ms/sample - loss: 0.0586 - acc: 0.9827 - val_loss: 0.0745 - val_acc: 0.9775\n",
      "Epoch 32/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0250 - acc: 0.9913\n",
      "Epoch 32: val_loss improved from 0.02431 to 0.02220, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0250 - acc: 0.9913 - val_loss: 0.0222 - val_acc: 0.9933\n",
      "Epoch 33/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9941\n",
      "Epoch 33: val_loss did not improve from 0.02220\n",
      "16000/16000 [==============================] - 42s 3ms/sample - loss: 0.0161 - acc: 0.9941 - val_loss: 0.0649 - val_acc: 0.9735\n",
      "Epoch 34/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0243 - acc: 0.9921\n",
      "Epoch 34: val_loss did not improve from 0.02220\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0243 - acc: 0.9921 - val_loss: 0.0346 - val_acc: 0.9893\n",
      "Epoch 35/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9958\n",
      "Epoch 35: val_loss improved from 0.02220 to 0.02001, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0200 - val_acc: 0.9940\n",
      "Epoch 36/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 36: val_loss did not improve from 0.02001\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0274 - val_acc: 0.9915\n",
      "Epoch 37/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9980\n",
      "Epoch 37: val_loss improved from 0.02001 to 0.01803, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0180 - val_acc: 0.9945\n",
      "Epoch 38/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.9909\n",
      "Epoch 38: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 42s 3ms/sample - loss: 0.0277 - acc: 0.9909 - val_loss: 0.0647 - val_acc: 0.9890\n",
      "Epoch 39/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1199 - acc: 0.9649\n",
      "Epoch 39: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1199 - acc: 0.9649 - val_loss: 0.0516 - val_acc: 0.9845\n",
      "Epoch 40/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0276 - acc: 0.9911\n",
      "Epoch 40: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0276 - acc: 0.9911 - val_loss: 0.0355 - val_acc: 0.9868\n",
      "Epoch 41/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0215 - acc: 0.9932\n",
      "Epoch 41: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0215 - acc: 0.9932 - val_loss: 0.0365 - val_acc: 0.9890\n",
      "Epoch 42/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9946\n",
      "Epoch 42: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0161 - acc: 0.9946 - val_loss: 0.0426 - val_acc: 0.9875\n",
      "Epoch 43/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9959\n",
      "Epoch 43: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0192 - val_acc: 0.9958\n",
      "Epoch 44/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9977\n",
      "Epoch 44: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0067 - acc: 0.9977 - val_loss: 0.0346 - val_acc: 0.9895\n",
      "Epoch 45/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0618 - acc: 0.9804\n",
      "Epoch 45: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0618 - acc: 0.9804 - val_loss: 0.0567 - val_acc: 0.9835\n",
      "Epoch 46/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0109 - acc: 0.9963\n",
      "Epoch 46: val_loss did not improve from 0.01803\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0243 - val_acc: 0.9933\n",
      "Epoch 47/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0151 - acc: 0.9958\n",
      "Epoch 47: val_loss improved from 0.01803 to 0.01388, saving model to ./model\\VGG_16_kfold_2.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0151 - acc: 0.9958 - val_loss: 0.0139 - val_acc: 0.9952\n",
      "Epoch 48/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9981\n",
      "Epoch 48: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0312 - val_acc: 0.9937\n",
      "Epoch 49/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0366 - acc: 0.9891\n",
      "Epoch 49: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0366 - acc: 0.9891 - val_loss: 0.0426 - val_acc: 0.9898\n",
      "Epoch 50/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0115 - acc: 0.9967\n",
      "Epoch 50: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0115 - acc: 0.9967 - val_loss: 0.0531 - val_acc: 0.9885\n",
      "Epoch 51/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0098 - acc: 0.9969\n",
      "Epoch 51: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0190 - val_acc: 0.9948\n",
      "Epoch 52/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 4.1313e-04 - acc: 0.9999\n",
      "Epoch 52: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 4.1313e-04 - acc: 0.9999 - val_loss: 0.0227 - val_acc: 0.9958\n",
      "Epoch 53/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0194 - acc: 0.9942\n",
      "Epoch 53: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0194 - acc: 0.9942 - val_loss: 0.0448 - val_acc: 0.9837\n",
      "Epoch 54/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0179 - acc: 0.9933\n",
      "Epoch 54: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0179 - acc: 0.9933 - val_loss: 0.0352 - val_acc: 0.9898\n",
      "Epoch 55/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0133 - acc: 0.9952\n",
      "Epoch 55: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0133 - acc: 0.9952 - val_loss: 0.0695 - val_acc: 0.9833\n",
      "Epoch 56/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0403 - acc: 0.9874\n",
      "Epoch 56: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0403 - acc: 0.9874 - val_loss: 0.0362 - val_acc: 0.9872\n",
      "Epoch 57/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0110 - acc: 0.9965\n",
      "Epoch 57: val_loss did not improve from 0.01388\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0265 - val_acc: 0.9937\n",
      "정확도(accuracy) :  0.99525\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fold : # 3\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.4097 - acc: 0.8169\n",
      "Epoch 1: val_loss improved from inf to 0.22239, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.4097 - acc: 0.8169 - val_loss: 0.2224 - val_acc: 0.9110\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1931 - acc: 0.9277\n",
      "Epoch 2: val_loss improved from 0.22239 to 0.13130, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1931 - acc: 0.9277 - val_loss: 0.1313 - val_acc: 0.9548\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1553 - acc: 0.9446\n",
      "Epoch 3: val_loss did not improve from 0.13130\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1553 - acc: 0.9446 - val_loss: 0.1548 - val_acc: 0.9435\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1223 - acc: 0.9556\n",
      "Epoch 4: val_loss improved from 0.13130 to 0.08939, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1223 - acc: 0.9556 - val_loss: 0.0894 - val_acc: 0.9700\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1052 - acc: 0.9622\n",
      "Epoch 5: val_loss did not improve from 0.08939\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1052 - acc: 0.9622 - val_loss: 0.0932 - val_acc: 0.9653\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0861 - acc: 0.9705\n",
      "Epoch 6: val_loss did not improve from 0.08939\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0861 - acc: 0.9705 - val_loss: 0.1378 - val_acc: 0.9515\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0938 - acc: 0.9639\n",
      "Epoch 7: val_loss did not improve from 0.08939\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0938 - acc: 0.9639 - val_loss: 0.1261 - val_acc: 0.9517\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0799 - acc: 0.9712\n",
      "Epoch 8: val_loss improved from 0.08939 to 0.07037, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0799 - acc: 0.9712 - val_loss: 0.0704 - val_acc: 0.9765\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0632 - acc: 0.9790\n",
      "Epoch 9: val_loss improved from 0.07037 to 0.06154, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0632 - acc: 0.9790 - val_loss: 0.0615 - val_acc: 0.9812\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0473 - acc: 0.9825\n",
      "Epoch 10: val_loss improved from 0.06154 to 0.03960, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0473 - acc: 0.9825 - val_loss: 0.0396 - val_acc: 0.9875\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0465 - acc: 0.9842\n",
      "Epoch 11: val_loss did not improve from 0.03960\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0465 - acc: 0.9842 - val_loss: 0.1512 - val_acc: 0.9433\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0633 - acc: 0.9780\n",
      "Epoch 12: val_loss improved from 0.03960 to 0.03882, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0633 - acc: 0.9780 - val_loss: 0.0388 - val_acc: 0.9865\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0410 - acc: 0.9854\n",
      "Epoch 13: val_loss did not improve from 0.03882\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0410 - acc: 0.9854 - val_loss: 0.0643 - val_acc: 0.9815\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0404 - acc: 0.9869\n",
      "Epoch 14: val_loss improved from 0.03882 to 0.03253, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0404 - acc: 0.9869 - val_loss: 0.0325 - val_acc: 0.9885\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0326 - acc: 0.9881\n",
      "Epoch 15: val_loss did not improve from 0.03253\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0326 - acc: 0.9881 - val_loss: 0.0476 - val_acc: 0.9830\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0452 - acc: 0.9856\n",
      "Epoch 16: val_loss did not improve from 0.03253\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0452 - acc: 0.9856 - val_loss: 0.1030 - val_acc: 0.9590\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0297 - acc: 0.9896\n",
      "Epoch 17: val_loss did not improve from 0.03253\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0297 - acc: 0.9896 - val_loss: 0.0678 - val_acc: 0.9795\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0383 - acc: 0.9881\n",
      "Epoch 18: val_loss did not improve from 0.03253\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0383 - acc: 0.9881 - val_loss: 0.0399 - val_acc: 0.9852\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0238 - acc: 0.9923\n",
      "Epoch 19: val_loss improved from 0.03253 to 0.01835, saving model to ./model\\VGG_16_kfold_3.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0183 - val_acc: 0.9937\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0312 - acc: 0.9895\n",
      "Epoch 20: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0312 - acc: 0.9895 - val_loss: 0.0545 - val_acc: 0.9872\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0351 - acc: 0.9896\n",
      "Epoch 21: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0351 - acc: 0.9896 - val_loss: 0.0504 - val_acc: 0.9847\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0355 - acc: 0.9889\n",
      "Epoch 22: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0355 - acc: 0.9889 - val_loss: 0.0274 - val_acc: 0.9927\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0215 - acc: 0.9927\n",
      "Epoch 23: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0215 - acc: 0.9927 - val_loss: 0.0246 - val_acc: 0.9925\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0327 - acc: 0.9893\n",
      "Epoch 24: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0409 - val_acc: 0.9847\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0120 - acc: 0.9962\n",
      "Epoch 25: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0273 - val_acc: 0.9915\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0506 - acc: 0.9844\n",
      "Epoch 26: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0506 - acc: 0.9844 - val_loss: 0.0436 - val_acc: 0.9835\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0233 - acc: 0.9930\n",
      "Epoch 27: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0233 - acc: 0.9930 - val_loss: 0.0289 - val_acc: 0.9920\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0418 - acc: 0.9873\n",
      "Epoch 28: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0418 - acc: 0.9873 - val_loss: 0.0217 - val_acc: 0.9937\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0105 - acc: 0.9970\n",
      "Epoch 29: val_loss did not improve from 0.01835\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0479 - val_acc: 0.9883\n",
      "정확도(accuracy) :  0.99375\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fold : # 4\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.5127 - acc: 0.7429\n",
      "Epoch 1: val_loss improved from inf to 0.23090, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 44s 3ms/sample - loss: 0.5127 - acc: 0.7429 - val_loss: 0.2309 - val_acc: 0.9022\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1910 - acc: 0.9271\n",
      "Epoch 2: val_loss improved from 0.23090 to 0.16215, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1910 - acc: 0.9271 - val_loss: 0.1621 - val_acc: 0.9410\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1551 - acc: 0.9405\n",
      "Epoch 3: val_loss improved from 0.16215 to 0.11744, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1551 - acc: 0.9405 - val_loss: 0.1174 - val_acc: 0.9550\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1378 - acc: 0.9473\n",
      "Epoch 4: val_loss did not improve from 0.11744\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1378 - acc: 0.9473 - val_loss: 0.1341 - val_acc: 0.9457\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1052 - acc: 0.9613\n",
      "Epoch 5: val_loss improved from 0.11744 to 0.09258, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1052 - acc: 0.9613 - val_loss: 0.0926 - val_acc: 0.9640\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0896 - acc: 0.9662\n",
      "Epoch 6: val_loss did not improve from 0.09258\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0896 - acc: 0.9662 - val_loss: 0.0957 - val_acc: 0.9657\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0718 - acc: 0.9746\n",
      "Epoch 7: val_loss improved from 0.09258 to 0.07142, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0718 - acc: 0.9746 - val_loss: 0.0714 - val_acc: 0.9740\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0670 - acc: 0.9756\n",
      "Epoch 8: val_loss did not improve from 0.07142\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0670 - acc: 0.9756 - val_loss: 0.0954 - val_acc: 0.9625\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0577 - acc: 0.9801\n",
      "Epoch 9: val_loss improved from 0.07142 to 0.05613, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0577 - acc: 0.9801 - val_loss: 0.0561 - val_acc: 0.9797\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0446 - acc: 0.9841\n",
      "Epoch 10: val_loss did not improve from 0.05613\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0446 - acc: 0.9841 - val_loss: 0.0590 - val_acc: 0.9800\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0954 - acc: 0.9664\n",
      "Epoch 11: val_loss did not improve from 0.05613\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0954 - acc: 0.9664 - val_loss: 0.0920 - val_acc: 0.9678\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0436 - acc: 0.9852\n",
      "Epoch 12: val_loss did not improve from 0.05613\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0436 - acc: 0.9852 - val_loss: 0.0577 - val_acc: 0.9790\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0431 - acc: 0.9849\n",
      "Epoch 13: val_loss did not improve from 0.05613\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0431 - acc: 0.9849 - val_loss: 0.0599 - val_acc: 0.9810\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0316 - acc: 0.9897\n",
      "Epoch 14: val_loss improved from 0.05613 to 0.05401, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0316 - acc: 0.9897 - val_loss: 0.0540 - val_acc: 0.9793\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.9844\n",
      "Epoch 15: val_loss improved from 0.05401 to 0.04176, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0441 - acc: 0.9844 - val_loss: 0.0418 - val_acc: 0.9843\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0301 - acc: 0.9898\n",
      "Epoch 16: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0301 - acc: 0.9898 - val_loss: 0.0773 - val_acc: 0.9740\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0488 - acc: 0.9836\n",
      "Epoch 17: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0488 - acc: 0.9836 - val_loss: 0.0702 - val_acc: 0.9743\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0678 - acc: 0.9776\n",
      "Epoch 18: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0678 - acc: 0.9776 - val_loss: 0.0870 - val_acc: 0.9765\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0288 - acc: 0.9901\n",
      "Epoch 19: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0288 - acc: 0.9901 - val_loss: 0.0474 - val_acc: 0.9865\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0258 - acc: 0.9912\n",
      "Epoch 20: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0258 - acc: 0.9912 - val_loss: 0.0425 - val_acc: 0.9858\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0237 - acc: 0.9913\n",
      "Epoch 21: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0237 - acc: 0.9913 - val_loss: 0.0777 - val_acc: 0.9797\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0352 - acc: 0.9887\n",
      "Epoch 22: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0352 - acc: 0.9887 - val_loss: 0.0605 - val_acc: 0.9803\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0436 - acc: 0.9862\n",
      "Epoch 23: val_loss did not improve from 0.04176\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0436 - acc: 0.9862 - val_loss: 0.0492 - val_acc: 0.9865\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0219 - acc: 0.9928\n",
      "Epoch 24: val_loss improved from 0.04176 to 0.04037, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0219 - acc: 0.9928 - val_loss: 0.0404 - val_acc: 0.9893\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0207 - acc: 0.9939\n",
      "Epoch 25: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0207 - acc: 0.9939 - val_loss: 0.0666 - val_acc: 0.9835\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0167 - acc: 0.9948\n",
      "Epoch 26: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0167 - acc: 0.9948 - val_loss: 0.0598 - val_acc: 0.9883\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0284 - acc: 0.9917\n",
      "Epoch 27: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0284 - acc: 0.9917 - val_loss: 0.0495 - val_acc: 0.9847\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0171 - acc: 0.9947\n",
      "Epoch 28: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0171 - acc: 0.9947 - val_loss: 0.1273 - val_acc: 0.9775\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0456 - acc: 0.9854\n",
      "Epoch 29: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0456 - acc: 0.9854 - val_loss: 0.0662 - val_acc: 0.9778\n",
      "Epoch 30/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0196 - acc: 0.9935\n",
      "Epoch 30: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0196 - acc: 0.9935 - val_loss: 0.0464 - val_acc: 0.9868\n",
      "Epoch 31/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0151 - acc: 0.9956\n",
      "Epoch 31: val_loss did not improve from 0.04037\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0151 - acc: 0.9956 - val_loss: 0.0748 - val_acc: 0.9840\n",
      "Epoch 32/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0122 - acc: 0.9958\n",
      "Epoch 32: val_loss improved from 0.04037 to 0.02925, saving model to ./model\\VGG_16_kfold_4.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0122 - acc: 0.9958 - val_loss: 0.0292 - val_acc: 0.9902\n",
      "Epoch 33/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 33: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0365 - val_acc: 0.9948\n",
      "Epoch 34/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 1.1359e-05 - acc: 1.0000\n",
      "Epoch 34: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 1.1359e-05 - acc: 1.0000 - val_loss: 0.0399 - val_acc: 0.9940\n",
      "Epoch 35/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 4.3415e-06 - acc: 1.0000\n",
      "Epoch 35: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 4.3415e-06 - acc: 1.0000 - val_loss: 0.0416 - val_acc: 0.9940\n",
      "Epoch 36/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 2.7955e-06 - acc: 1.0000\n",
      "Epoch 36: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 2.7955e-06 - acc: 1.0000 - val_loss: 0.0429 - val_acc: 0.9940\n",
      "Epoch 37/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 1.9430e-06 - acc: 1.0000\n",
      "Epoch 37: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 1.9430e-06 - acc: 1.0000 - val_loss: 0.0441 - val_acc: 0.9940\n",
      "Epoch 38/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 1.3861e-06 - acc: 1.0000\n",
      "Epoch 38: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 1.3861e-06 - acc: 1.0000 - val_loss: 0.0451 - val_acc: 0.9940\n",
      "Epoch 39/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 1.0245e-06 - acc: 1.0000\n",
      "Epoch 39: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 1.0245e-06 - acc: 1.0000 - val_loss: 0.0459 - val_acc: 0.9940\n",
      "Epoch 40/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 7.7120e-07 - acc: 1.0000\n",
      "Epoch 40: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 7.7120e-07 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9940\n",
      "Epoch 41/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 6.0060e-07 - acc: 1.0000\n",
      "Epoch 41: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 6.0060e-07 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9940\n",
      "Epoch 42/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 4.7114e-07 - acc: 1.0000\n",
      "Epoch 42: val_loss did not improve from 0.02925\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 4.7114e-07 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9940\n",
      "정확도(accuracy) :  0.99025\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Fold : # 5\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.7389 - acc: 0.6066\n",
      "Epoch 1: val_loss improved from inf to 0.52880, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 45s 3ms/sample - loss: 0.7389 - acc: 0.6066 - val_loss: 0.5288 - val_acc: 0.7498\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.3071 - acc: 0.8718\n",
      "Epoch 2: val_loss improved from 0.52880 to 0.23509, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.3071 - acc: 0.8718 - val_loss: 0.2351 - val_acc: 0.9068\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1740 - acc: 0.9339\n",
      "Epoch 3: val_loss improved from 0.23509 to 0.16157, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1740 - acc: 0.9339 - val_loss: 0.1616 - val_acc: 0.9375\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.9480\n",
      "Epoch 4: val_loss improved from 0.16157 to 0.11716, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1356 - acc: 0.9480 - val_loss: 0.1172 - val_acc: 0.9535\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.1209 - acc: 0.9553\n",
      "Epoch 5: val_loss did not improve from 0.11716\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.1209 - acc: 0.9553 - val_loss: 0.1660 - val_acc: 0.9423\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0951 - acc: 0.9651\n",
      "Epoch 6: val_loss did not improve from 0.11716\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0951 - acc: 0.9651 - val_loss: 0.2250 - val_acc: 0.9215\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0913 - acc: 0.9674\n",
      "Epoch 7: val_loss improved from 0.11716 to 0.09728, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0913 - acc: 0.9674 - val_loss: 0.0973 - val_acc: 0.9643\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0726 - acc: 0.9739\n",
      "Epoch 8: val_loss improved from 0.09728 to 0.08362, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0726 - acc: 0.9739 - val_loss: 0.0836 - val_acc: 0.9740\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0995 - acc: 0.9624\n",
      "Epoch 9: val_loss did not improve from 0.08362\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0995 - acc: 0.9624 - val_loss: 0.0893 - val_acc: 0.9740\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0686 - acc: 0.9750\n",
      "Epoch 10: val_loss improved from 0.08362 to 0.07416, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0686 - acc: 0.9750 - val_loss: 0.0742 - val_acc: 0.9728\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0533 - acc: 0.9811\n",
      "Epoch 11: val_loss did not improve from 0.07416\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0533 - acc: 0.9811 - val_loss: 0.0885 - val_acc: 0.9703\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0520 - acc: 0.9803\n",
      "Epoch 12: val_loss did not improve from 0.07416\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0520 - acc: 0.9803 - val_loss: 0.1029 - val_acc: 0.9682\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0471 - acc: 0.9822\n",
      "Epoch 13: val_loss did not improve from 0.07416\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0471 - acc: 0.9822 - val_loss: 0.0776 - val_acc: 0.9732\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9816\n",
      "Epoch 14: val_loss improved from 0.07416 to 0.05069, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0504 - acc: 0.9816 - val_loss: 0.0507 - val_acc: 0.9837\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0377 - acc: 0.9870\n",
      "Epoch 15: val_loss improved from 0.05069 to 0.03367, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0377 - acc: 0.9870 - val_loss: 0.0337 - val_acc: 0.9895\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0329 - acc: 0.9883\n",
      "Epoch 16: val_loss did not improve from 0.03367\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0329 - acc: 0.9883 - val_loss: 0.0443 - val_acc: 0.9843\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0669 - acc: 0.9768\n",
      "Epoch 17: val_loss did not improve from 0.03367\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0669 - acc: 0.9768 - val_loss: 0.0978 - val_acc: 0.9613\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0480 - acc: 0.9832\n",
      "Epoch 18: val_loss did not improve from 0.03367\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0480 - acc: 0.9832 - val_loss: 0.0464 - val_acc: 0.9803\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0591 - acc: 0.9816\n",
      "Epoch 19: val_loss did not improve from 0.03367\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0591 - acc: 0.9816 - val_loss: 0.0485 - val_acc: 0.9810\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0301 - acc: 0.9892\n",
      "Epoch 20: val_loss improved from 0.03367 to 0.03264, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0301 - acc: 0.9892 - val_loss: 0.0326 - val_acc: 0.9885\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0292 - acc: 0.9899\n",
      "Epoch 21: val_loss did not improve from 0.03264\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0292 - acc: 0.9899 - val_loss: 0.0764 - val_acc: 0.9722\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0184 - acc: 0.9934\n",
      "Epoch 22: val_loss improved from 0.03264 to 0.02374, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0184 - acc: 0.9934 - val_loss: 0.0237 - val_acc: 0.9912\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0194 - acc: 0.9934\n",
      "Epoch 23: val_loss did not improve from 0.02374\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0194 - acc: 0.9934 - val_loss: 0.0519 - val_acc: 0.9858\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0274 - acc: 0.9901\n",
      "Epoch 24: val_loss did not improve from 0.02374\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0274 - acc: 0.9901 - val_loss: 0.1006 - val_acc: 0.9762\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0266 - acc: 0.9911\n",
      "Epoch 25: val_loss did not improve from 0.02374\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0266 - acc: 0.9911 - val_loss: 0.0511 - val_acc: 0.9860\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0205 - acc: 0.9927\n",
      "Epoch 26: val_loss did not improve from 0.02374\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0205 - acc: 0.9927 - val_loss: 0.0370 - val_acc: 0.9925\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0256 - acc: 0.9927\n",
      "Epoch 27: val_loss did not improve from 0.02374\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0256 - acc: 0.9927 - val_loss: 0.0252 - val_acc: 0.9920\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0121 - acc: 0.9956\n",
      "Epoch 28: val_loss improved from 0.02374 to 0.02344, saving model to ./model\\VGG_16_kfold_5.hdf5\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0121 - acc: 0.9956 - val_loss: 0.0234 - val_acc: 0.9918\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0169 - acc: 0.9942\n",
      "Epoch 29: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0169 - acc: 0.9942 - val_loss: 0.0324 - val_acc: 0.9880\n",
      "Epoch 30/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0391 - acc: 0.9875\n",
      "Epoch 30: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0391 - acc: 0.9875 - val_loss: 0.0858 - val_acc: 0.9747\n",
      "Epoch 31/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0309 - acc: 0.9899\n",
      "Epoch 31: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0309 - acc: 0.9899 - val_loss: 0.0308 - val_acc: 0.9898\n",
      "Epoch 32/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0158 - acc: 0.9951\n",
      "Epoch 32: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0262 - val_acc: 0.9883\n",
      "Epoch 33/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0351 - acc: 0.9891\n",
      "Epoch 33: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0351 - acc: 0.9891 - val_loss: 0.0402 - val_acc: 0.9862\n",
      "Epoch 34/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0172 - acc: 0.9948\n",
      "Epoch 34: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0172 - acc: 0.9948 - val_loss: 0.0479 - val_acc: 0.9865\n",
      "Epoch 35/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0186 - acc: 0.9935\n",
      "Epoch 35: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0186 - acc: 0.9935 - val_loss: 0.0332 - val_acc: 0.9893\n",
      "Epoch 36/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0188 - acc: 0.9948\n",
      "Epoch 36: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0188 - acc: 0.9948 - val_loss: 0.0785 - val_acc: 0.9743\n",
      "Epoch 37/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0512 - acc: 0.9840\n",
      "Epoch 37: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0512 - acc: 0.9840 - val_loss: 0.0640 - val_acc: 0.9770\n",
      "Epoch 38/100\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0208 - acc: 0.9933\n",
      "Epoch 38: val_loss did not improve from 0.02344\n",
      "16000/16000 [==============================] - 43s 3ms/sample - loss: 0.0208 - acc: 0.9933 - val_loss: 0.2373 - val_acc: 0.9540\n",
      "정확도(accuracy) :  0.99175\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1fold': 0.991,\n",
       " '2fold': 0.99525,\n",
       " '3fold': 0.99375,\n",
       " '4fold': 0.99025,\n",
       " '5fold': 0.99175}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "idx = 1\n",
    "\n",
    "fold_accuracy = {}\n",
    "\n",
    "for train, val in kf.split(x,y) :\n",
    "    print(\"Fold : #\", idx)\n",
    "    \n",
    "    VGG = VGG16(input_shape=(128, 128, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "    VGG.trainable = True\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(VGG)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=256,activation=\"relu\"))\n",
    "    model.add(Dense(units=128,activation=\"relu\"))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    modelpath = './model/'\n",
    "    if not os.path.exists(modelpath) :\n",
    "        os.mkdir(modelpath)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath = modelpath + f'VGG_16_kfold_{idx}.hdf5', monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience=10)\n",
    "\n",
    "    history = model.fit(x[train], y[train], validation_data=[x[val], y[val]], epochs = 100,  batch_size = 64, callbacks = [checkpointer, early_stopping_callback])\n",
    "    \n",
    "    best_model = load_model(f'./model/VGG_16_kfold_{idx}.hdf5')\n",
    "    # 정확도(accuracy)\n",
    "    print('정확도(accuracy) : ', best_model.evaluate(x[val], y[val])[1])\n",
    "    \n",
    "    fold_accuracy[f'{idx}fold'] = best_model.evaluate(x[val], y[val])[1]\n",
    "        \n",
    "    print('-'*100)\n",
    "    idx += 1\n",
    "\n",
    "fold_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923999905586243"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fold_accuracy.values())/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy(%)</th>\n",
       "      <th>image_size</th>\n",
       "      <th>eopchs</th>\n",
       "      <th>batchsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VGG_16_KFold</td>\n",
       "      <td>99.24</td>\n",
       "      <td>(128, 128)</td>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG_16</td>\n",
       "      <td>99.28</td>\n",
       "      <td>(128, 128)</td>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  accuracy(%)  image_size  eopchs  batchsize\n",
       "0  VGG_16_KFold        99.24  (128, 128)     100         64\n",
       "1        VGG_16        99.28  (128, 128)     100         64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['model', 'accuracy(%)', 'image_size', 'eopchs', 'batchsize'])\n",
    "df.loc[0] = ['VGG_16_KFold', 99.24, (128, 128), 100, 64]\n",
    "df.loc[1] = ['VGG_16',  99.28, (128, 128), 100, 64]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('models.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3977  3978  3980 ... 19997 19998 19999]\n",
      "[   0    1    2 ... 4024 4025 4027]\n",
      "[    0     1     2 ... 19997 19998 19999]\n",
      "[3977 3978 3980 ... 8032 8033 8037]\n",
      "[    0     1     2 ... 19997 19998 19999]\n",
      "[ 7962  7963  7966 ... 12089 12090 12091]\n",
      "[    0     1     2 ... 19997 19998 19999]\n",
      "[11904 11909 11914 ... 16015 16016 16018]\n",
      "[    0     1     2 ... 16015 16016 16018]\n",
      "[15986 15987 15988 ... 19997 19998 19999]\n"
     ]
    }
   ],
   "source": [
    "for train, val in kf.split(x,y) :\n",
    "    \n",
    "    print(train)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
