{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I-wSpv_8GlRK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "Gxl5RqvyG1OO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_to_word_sequence() 함수\n",
        "\n",
        "#전처리 할 텍스트트\n",
        "text = '해보지 않으면 해낼 수 없다. 당신이 선택한 의사결정을 후회하지 말라'\n",
        "\n",
        "#해당 텍스트를 토큰화화 문장을 단어 단위로 토큰화 해줌줌\n",
        "result = text_to_word_sequence(text)\n",
        "print('원문:\\n ', text)\n",
        "print('\\n토큰화:\\n ', result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjGgLJhWHXDF",
        "outputId": "b581af30-808b-47cf-8ef5-051910b9ced5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문:\n",
            "  해보지 않으면 해낼 수 없다. 당신이 선택한 의사결정을 후회하지 말라\n",
            "\n",
            "토큰화:\n",
            "  ['해보지', '않으면', '해낼', '수', '없다', '당신이', '선택한', '의사결정을', '후회하지', '말라']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 빈도 세기 \n",
        "#전처리 하려는 세 개 의 문장 정하기\n",
        "\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
        "'텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "'토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n",
        "]\n",
        "\n",
        "#텍스트의 토큰화 (전처리)\n",
        "\n",
        "token=Tokenizer() #토큰화 함수 사용\n",
        "token.fit_on_texts(docs) #토큰화 함수를 문장에 적용\n",
        "\n",
        "#단어 빈도수 계산 > 각 옵션에 맞게 출력\n",
        "\n",
        "print('단어 카운트:\\n', token.word_counts) #단어의 빈도 세어 줌\n",
        "print('\\n문장 카운트:\\n', token.document_count) #문장장의 빈도 세어 줌\n",
        "print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs) \n",
        "print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv3V5hvyH-DH",
        "outputId": "2c3d7c89-9a90-4369-a79f-72e381844f66"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 카운트:\n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 1), ('합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트:\n",
            " 3\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'단어를': 1, '먼저': 1, '합니다': 1, '각': 1, '텍스트의': 2, '토큰화': 1, '나누어': 1, '인식됩니다': 1, '딥러닝에서': 2, '토큰화해야': 1, '단어로': 1, '사용할': 1, '결과는': 1, '있습니다': 1, '토큰화한': 1, '수': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화': 7, '합니다': 8, '단어로': 9, '토큰화해야': 10, '인식됩니다': 11, '토큰화한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어의 원-핫 인코딩\n",
        "\n",
        "text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts([text])\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOfyJzzIJGAv",
        "outputId": "811263b9-55bc-4af4-ed16-9f4eb384fe88"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=token.texts_to_sequences([text])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk3_gLsAMANV",
        "outputId": "20b791e3-a7cb-49eb-f970-880278016902"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#index +1 1 원핫 인코딩 배열 만들기 앞에 0이 있어야 하니 인덱스 숫자 맞춰야해서서 +1 꼭 추가\n",
        "\n",
        "word_size = len(token.word_index) +1\n",
        "word_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8wiJEVzMEEG",
        "outputId": "cb3490ef-df7c-4a1a-bb3e-830ddd3474c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_categorical(x, num_classes=word_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9i6Ed4sMSgN",
        "outputId": "ac830c47-dccd-4a7c-8b38-a62c40f644d2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#텍스트를 읽고 긍정 부정 예측하기\n",
        "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]"
      ],
      "metadata": {
        "id": "bf9bP52YRrPa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#긍정 리뷰 1 부정 리뷰 0\n",
        "\n",
        "classes = np.array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "WJc9obFhRrMy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdISWAaeRrKK",
        "outputId": "1a38fc59-ecb0-4e2c-85ab-bd7187c5c204"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=token.texts_to_sequences(docs)\n",
        "x #토큰화 결과"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co11DLHYRrDL",
        "outputId": "c92be39c-d9fa-457c-eadb-4da7eef9efbb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2],\n",
              " [3],\n",
              " [4, 5, 6, 7],\n",
              " [8, 9, 10],\n",
              " [11, 12, 13],\n",
              " [14],\n",
              " [15],\n",
              " [16, 17],\n",
              " [18, 19],\n",
              " [20]]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어의 길이를 맞추어 주어야 함 > padding (zero padding)\n",
        "#x라는 문장의 4로 길이를 맞추어줘\n",
        "\n",
        "pad_sequences(x, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOJxFDdmSL9U",
        "outputId": "f6fba6d0-5a33-44de-8c5e-562a5d786c9c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  1,  2],\n",
              "       [ 0,  0,  0,  3],\n",
              "       [ 4,  5,  6,  7],\n",
              "       [ 0,  8,  9, 10],\n",
              "       [ 0, 11, 12, 13],\n",
              "       [ 0,  0,  0, 14],\n",
              "       [ 0,  0,  0, 15],\n",
              "       [ 0,  0, 16, 17],\n",
              "       [ 0,  0, 18, 19],\n",
              "       [ 0,  0,  0, 20]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_x = pad_sequences(x, 4)\n",
        "print('패딩 결과:\\n', padded_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oryhNKeXS8GC",
        "outputId": "c6706aea-b73e-483c-ab21-88c6491c622c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "패딩 결과:\n",
            " [[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [ 0 11 12 13]\n",
            " [ 0  0  0 14]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0 16 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0  0 20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩에 입력될 단어의 수 지정\n",
        "\n",
        "word_size = len(token.word_index) +1\n",
        "#인덱스는 0부터 시작하기 때문에 길이를 맞춰줌 (+1)\n",
        "\n",
        "#단어 임베딩 포함, 딥러닝 모델 만들기\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(word_size, 16, input_length=4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "#8에 인덱스 20 곱한다믄 8 더해주기\n",
        "#차원 축소 다음 32+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmQgziW8SL65",
        "outputId": "d311fc66-e302-4bf8-d18c-66e1cc58b5f8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 4, 16)             336       \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 401\n",
            "Trainable params: 401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(padded_x, classes, epochs=20)\n",
        "print('\\n ACC: %.4f' %(model.evaluate(padded_x, classes)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tydb9PZaSL4g",
        "outputId": "e9608d01-4093-4127-e66e-8724e08856ca"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 1s 934ms/step - loss: 0.6956 - accuracy: 0.4000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6925 - accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6894 - accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6864 - accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6833 - accuracy: 0.6000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6803 - accuracy: 0.7000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6773 - accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6742 - accuracy: 0.8000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6712 - accuracy: 0.9000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6682 - accuracy: 0.9000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6651 - accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6621 - accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6590 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6560 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6529 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6498 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6468 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6437 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6406 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6374 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.6343 - accuracy: 1.0000\n",
            "\n",
            " ACC: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_hUl5jrYSL15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iEIgUEPSLzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HeoDCIwSLwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECbtaDwkSLuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding"
      ],
      "metadata": {
        "id": "TbdfumFlMrp1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#단어 임베딩\n"
      ],
      "metadata": {
        "id": "uC2Dd9EANeAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}